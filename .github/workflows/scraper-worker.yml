name: College Scraper Worker

on:
  # Run every 30 minutes to check for pending jobs
  schedule:
    - cron: '*/30 * * * *'
  
  # Trigger on push to nayan-dev (for testing)
  push:
    branches:
      - nayan-dev
    paths:
      - '.github/workflows/scraper-worker.yml'
  
  # Allow manual trigger from GitHub UI or API
  workflow_dispatch:
    inputs:
      job_timeout:
        description: 'Timeout in minutes for the scraping job'
        required: false
        default: '30'

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 60  # Max 6 hours allowed, we use 60 min to be safe
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python 3.12
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'
          cache: 'pip'
          cache-dependency-path: 'backend/scraping-service/requirements.txt'
      
      - name: Install Python dependencies
        run: |
          cd backend/scraping-service
          pip install -r requirements.txt
      
      - name: Install Playwright browsers
        run: |
          playwright install chromium --with-deps
      
      - name: Run Scraper Worker
        env:
          # Supabase credentials (set these in GitHub Secrets)
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}

          # Enable serverless mode (skip local file writes)
          SERVERLESS_MODE: "true"
        run: |
          cd backend/scraping-service
          python worker.py
      
      - name: Upload logs on failure
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-logs-${{ github.run_id }}
          path: backend/scraping-service/*.log
          retention-days: 7
